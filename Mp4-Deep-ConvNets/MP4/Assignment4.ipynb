{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N74aqM_ZpK7V"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"CS543_MP4_part1_starter_code.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1K75i8TjPhHM6bhr2y3uESkp2nIzyvbnW\n",
        "\n",
        "# Google Colab setup with Google Drive folder\n",
        "\n",
        "This notebook provides the code you need to set up Google Colab to run and import files from within a Google Drive folder.\n",
        "\n",
        "This will allow you to upload assignment code to your Google Drive and then run the code on Google Colab machines (with free GPUs if needed).\n",
        "\n",
        "You will need to create a folder in your Google Drive to hold your assignments and you will need to open Colaboratory within this folder before running the set up code (check the link above to see how).\n",
        "\n",
        "Note: this use of Google Drive is optional, and you could also just manually copy the data into your colab runtime. Keep in mind, this won't be persistent though, and you will have to download your models / plots before the runtime shuts down.\n",
        "\n",
        "# Mount Google Drive\n",
        "\n",
        "This will allow the Colab machine to access Google Drive folders by mounting the drive on the machine. You will be asked to copy and paste an authentication code.\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "\"\"\"# Change directory to allow imports\n",
        "\n",
        "\n",
        "As noted above, you should create a Google Drive folder to hold all your assignment files. You will need to add this code to the top of any python notebook you run to be able to import python files from your drive assignment folder (you should change the file path below to be your own assignment folder).\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/CS543/MP4\"):\n",
        "    os.makedirs(\"/content/gdrive/My Drive/Colab Notebooks/CS_543_MP4\")\n",
        "os.chdir(\"/content/gdrive/MyDrive/CS543/MP4\")\n",
        "\n",
        "!ls # Check if this is your MP4 folder\n",
        "\n",
        "\"\"\"# Set up GPU and PyTorch\n",
        "\n",
        "First, ensure that your notebook on Colaboratory is set up to use GPU. After opening the notebook on Colaboratory, go to Edit>Notebook settings, select Python 3 under \"Runtime type,\" select GPU under \"Hardware accelerator,\" and save.\n",
        "\n",
        "Next, install PyTorch:\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install torch torchvision\n",
        "\n",
        "\"\"\"Make sure that pytorch is installed and works with GPU:\"\"\"\n",
        "\n",
        "import torch\n",
        "a = torch.Tensor([1]).cuda()\n",
        "print(a)\n",
        "\n",
        "torch.cuda.is_available()\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# imports and useful functions\n",
        "\n",
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import sys\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "import torch.utils.data as data\n",
        "from torchvision.datasets.utils import download_url, check_integrity\n",
        "import copy\n",
        "import csv\n",
        "import matplotlib\n",
        "# %matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os.path\n",
        "from tqdm.notebook import tqdm\n",
        "import sys\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(111)\n",
        "torch.cuda.manual_seed_all(111)\n",
        "torch.manual_seed(111)\n",
        "\n",
        "TOTAL_CLASSES = 10\n",
        "class FashionMNISTTest(torchvision.datasets.VisionDataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super(FashionMNISTTest, self).__init__(root, transform=transform)\n",
        "\n",
        "        image_filename = os.path.join(root, 'fashion_secret_test_data.npy')\n",
        "        images = np.load(image_filename)\n",
        "\n",
        "        assert len(images.shape) == 3\n",
        "        assert images.shape[0] == 2000\n",
        "        assert images.shape[1] == 28\n",
        "        assert images.shape[2] == 28\n",
        "        self.data = images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def calculate_accuracy(dataloader, model, is_gpu):\n",
        "    \"\"\" Util function to calculate val set accuracy,\n",
        "    both overall and per class accuracy\n",
        "    Args:\n",
        "        dataloader (torch.utils.data.DataLoader): val set\n",
        "        is_gpu (bool): whether to run on GPU\n",
        "    Returns:\n",
        "        tuple: (overall accuracy, class level accuracy)\n",
        "    \"\"\"\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    predictions = []\n",
        "\n",
        "    class_correct = list(0. for i in range(TOTAL_CLASSES))\n",
        "    class_total = list(0. for i in range(TOTAL_CLASSES))\n",
        "\n",
        "    # Check out why .eval() is important!\n",
        "    # https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744/2\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            if is_gpu:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "            outputs = model(Variable(images))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predictions.extend(list(predicted.cpu().numpy()))\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    class_accuracy = 100 * np.divide(class_correct, class_total)\n",
        "    return 100 * correct / total, class_accuracy\n",
        "\n",
        "\n",
        "def run_secret_test(dataloader, model, is_gpu):\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for images in dataloader:\n",
        "          if is_gpu:\n",
        "              images = images.cuda()\n",
        "          outputs = model(Variable(images))\n",
        "          predicted = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "          predictions.extend(list(predicted))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\"\"\"\n",
        "Training an image classifier\n",
        "----------------------------\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Load the randomized FashionMNIST training, validation and test datasets using\n",
        "   torchvision. Use torchvision.transforms to apply transforms on the\n",
        "   dataset.\n",
        "2. Define a Convolution Neural Network - BaseNet\n",
        "3. Define a loss function and optimizer\n",
        "4. Train the network on training data and check performance on val set.\n",
        "   Plot train loss and validation accuracies.\n",
        "5. Try the network on test data and create .npy file for submission to Gradescope\"\"\"\n",
        "\n",
        "# <<TODO>>: Based on the val set performance, decide how many\n",
        "# epochs are apt for your model.\n",
        "# ---------\n",
        "EPOCHS = 30\n",
        "# ---------\n",
        "IS_GPU = True\n",
        "TEST_BS = 256\n",
        "TOTAL_CLASSES = 10\n",
        "TRAIN_BS = 64\n",
        "PATH_TO_Fashion = \"data/fashion/\"\n",
        "PATH_TO_Fashion_TEST = \"data/fashion/\"\n",
        "\n",
        "\"\"\"1.**Loading FashionMNIST**\n",
        "\n",
        "We will load the FashionMNIST dataset with builtin dataset loader from Torchvision.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# # Using transforms.ToTensor(), transform them to Tensors of normalized range\n",
        "# # [-1, 1].\n",
        "\n",
        "\n",
        "# # <<TODO#1>> Use transforms.Normalize() with the right parameters to\n",
        "# # make the data well conditioned (zero mean, std dev=1) for improved training.\n",
        "# # <<TODO#2>> Try using transforms.RandomCrop() and/or transforms.RandomHorizontalFlip()\n",
        "# # to augment training data.\n",
        "# # After your edits, make sure that test_transform should have the same data\n",
        "# # normalization parameters as train_transform\n",
        "# # You shouldn't have any data augmentation in test_transform (val or test data is never augmented).\n",
        "# # ---------------------\n",
        "# train_transform = transforms.Compose([\n",
        "#     #transforms.RandomCrop(28, padding=2),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.2858,), (0.3204,))  # normalize to have zero mean and unit standard deviation\n",
        "# ])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.2858,), (0.3204,))  # use the same normalization parameters as train_transform\n",
        "# ])\n",
        "# # ---------------------\n",
        "\n",
        "# #DO NOT CHANGE any line below\n",
        "# dataset = torchvision.datasets.FashionMNIST(root=PATH_TO_Fashion, train=True, download=True, transform=train_transform)\n",
        "# print(\"dataset data shape: \", np.array(dataset.data).shape)\n",
        "# print(\"dataset labels shape: \", np.array(dataset.targets).shape)\n",
        "\n",
        "# # The 10 classes for FashionMNIST\n",
        "# classes = dataset.classes\n",
        "# # train dataset 55, 000 samples\n",
        "# train_dataset = torch.utils.data.Subset(dataset, np.arange(len(dataset)-5000))\n",
        "# # val dataset 5, 000 samples\n",
        "# val_dataset = torch.utils.data.Subset(dataset, np.arange(len(dataset)-5000, len(dataset)))\n",
        "\n",
        "# # check for Dataloader function: https://pytorch.org/docs/stable/data.html\n",
        "# trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BS, shuffle=True, num_workers=2, drop_last=True)  #DO NOT CHANGE\n",
        "# valloader = torch.utils.data.DataLoader(val_dataset, batch_size=TEST_BS, shuffle=False, num_workers=2, drop_last=False)\n",
        "\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# Using transforms.ToTensor(), transform them to Tensors of normalized range\n",
        "# [-1, 1].\n",
        "\n",
        "\n",
        "# <<TODO#1>> Use transforms.Normalize() with the right parameters to\n",
        "# make the data well conditioned (zero mean, std dev=1) for improved training.\n",
        "# <<TODO#2>> Try using transforms.RandomCrop() and/or transforms.RandomHorizontalFlip()\n",
        "# to augment training data.\n",
        "# After your edits, make sure that test_transform should have the same data\n",
        "# normalization parameters as train_transform\n",
        "# You shouldn't have any data augmentation in test_transform (val or test data is never augmented).\n",
        "# ---------------------\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "# ---------------------\n",
        "\n",
        "#DO NOT CHANGE any line below\n",
        "dataset = torchvision.datasets.FashionMNIST(root=PATH_TO_Fashion, train=True, download=True, transform=train_transform)\n",
        "print(\"dataset data shape: \", np.array(dataset.data).shape)\n",
        "print(\"dataset labels shape: \", np.array(dataset.targets).shape)\n",
        "\n",
        "# The 10 classes for FashionMNIST\n",
        "classes = dataset.classes\n",
        "# train dataset 55, 000 samples\n",
        "train_dataset = torch.utils.data.Subset(dataset, np.arange(len(dataset)-5000))\n",
        "# val dataset 5, 000 samples\n",
        "val_dataset = torch.utils.data.Subset(dataset, np.arange(len(dataset)-5000, len(dataset)))\n",
        "\n",
        "# check for Dataloader function: https://pytorch.org/docs/stable/data.html\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BS, shuffle=True, num_workers=2, drop_last=True)  #DO NOT CHANGE\n",
        "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=TEST_BS, shuffle=False, num_workers=2, drop_last=False)\n",
        "\n",
        "# mean = 0.\n",
        "# std = 0.\n",
        "# num_samples = 0.\n",
        "# for data, _ in trainloader:\n",
        "#     batch_size = data.size(0)\n",
        "#     data = data.view(batch_size, data.size(1), -1)\n",
        "#     mean += data.mean(2).sum(0)\n",
        "#     std += data.std(2).sum(0)\n",
        "#     num_samples += batch_size\n",
        "\n",
        "# mean /= num_samples\n",
        "# std /= num_samples\n",
        "\n",
        "# print(\"Mean: \", mean)\n",
        "# print(\"Std: \", std)\n",
        "\n",
        "# mean = 0.\n",
        "# std = 0.\n",
        "# num_samples = 0.\n",
        "# for data, _ in valloader:\n",
        "#     batch_size = data.size(0)\n",
        "#     data = data.view(batch_size, data.size(1), -1)\n",
        "#     mean += data.mean(2).sum(0)\n",
        "#     std += data.std(2).sum(0)\n",
        "#     num_samples += batch_size\n",
        "\n",
        "# mean /= num_samples\n",
        "# std /= num_samples\n",
        "\n",
        "# print(\"Mean: \", mean)\n",
        "# print(\"Std: \", std)\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "#Load secret test set\n",
        "#DO NOT CHANGE any line below\n",
        "if not os.path.isfile(os.path.join(\"/content/gdrive/My Drive/Colab Notebooks/CS_543_MP4\", PATH_TO_Fashion_TEST, \"fashion_secret_test_data.npy\")):\n",
        "#   %pip install -U gdown\n",
        "  import gdown\n",
        "  url = \"https://drive.google.com/uc?id=130ssD7hrrDLqQHKVD3luD0tXTedWjC8j\"\n",
        "  output = os.path.join(PATH_TO_Fashion_TEST, \"fashion_secret_test_data.npy\")\n",
        "  gdown.download(url, output, quiet=False)\n",
        "\n",
        "test_img = np.load(os.path.join(PATH_TO_Fashion_TEST, \"fashion_secret_test_data.npy\"))\n",
        "test_dataset = FashionMNISTTest(PATH_TO_Fashion_TEST, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BS, shuffle=False, num_workers=2, drop_last=False) #DO NOT CHANGE\n",
        "\n",
        "\"\"\"2.**Visualize FashionMNIST**\n",
        "\n",
        "We will visualize some random images from the FashionMNIST dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Let us show some of the training images, for fun.\n",
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:16]))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(16)))\n",
        "\n",
        "\"\"\"3.**Define a Convolution Neural Network**\n",
        "\n",
        "Implement the BaseNet exactly. BaseNet consists of two convolutional modules (conv-relu-maxpool) and two linear layers. The precise architecture is defined below:\n",
        "\n",
        "| Layer No.   | Layer Type  | Kernel Size | Input Dim   | Output Dim  | Input Channels | Output Channels |\n",
        "    | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
        "    | 1 | conv2d | 5 | 28 | 24 | 1 | 6 |\n",
        "         | 2 | relu | - | 24 | 24 | 6 | 6 |\n",
        "         | 3 | maxpool2d | 2 | 24 | 12 | 6 | 6 |\n",
        "         | 4 | conv2d | 5 | 12 | 8 | 6 | 12 |\n",
        "         | 5 | relu | - | 8 | 8 | 12 | 12 |\n",
        "         | 6 | maxpool2d | 2 | 8 | 4 | 12 | 12 |\n",
        "         | 7 | linear | - | 1 | 1 | 192 | 12 |\n",
        "         | 8 | relu | - | 1 | 1 | 12 | 12 |\n",
        "         | 9 | linear | - | 1 | 1 | 12 | 10 |\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseNet, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=12)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=12, out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: define your model here\n",
        "        x = self.relu1(self.conv1(x)) # Size([bs, 6, 24, 24])\n",
        "        x = self.pool1(x)             # Size([bs, 6, 12, 12])\n",
        "\n",
        "        # Second convolutional layer\n",
        "        x = self.relu2(self.conv2(x)) # Size([bs, 12, 8, 8])\n",
        "        x = self.pool2(x)             # Size([bs, 12, 4, 4])\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        x = x.view(-1, 12 * 4 * 4)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)               # Size([bs, 192])\n",
        "        x = self.relu3(x)             # size([bs, 12])\n",
        "        x = self.fc2(x)               #size([bs, 10])\n",
        "        return x\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseNet, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "        self.bn2 = nn.BatchNorm2d(12)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=96)\n",
        "        self.bn3 = nn.BatchNorm1d(12)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=96, out_features=48)\n",
        "        self.fc3 = nn.Linear(48, 24)\n",
        "        self.fc4 = nn.Linear(24, 10)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.drop1 =nn.Dropout(0.20)\n",
        "        self.drop2 = nn.Dropout(0.20)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: define your model here\n",
        "        x = self.relu1(self.bn1(self.conv1(x))) # Size([bs, 6, 24, 24])\n",
        "        x = self.pool1(x)             # Size([bs, 6, 12, 12])\n",
        "        x = self.drop1(x)\n",
        "        # Second convolutional layer\n",
        "        x = self.relu2(self.bn2(self.conv2(x))) # Size([bs, 12, 8, 8])\n",
        "        x = self.pool2(x)             # Size([bs, 12, 4, 4])\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        x = x.view(-1, 12 * 4 * 4)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)               # Size([bs, 192])\n",
        "        x = self.relu3(x)             # size([bs, 12])\n",
        "        x = self.drop2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.fc4(x)               #size([bs, 10])\n",
        "        return x\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)  # chnl-in, out, kernel\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.fc1 = nn.Linear(1024, 512)   # [64*4*4, x]\n",
        "        self.bn3 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        #self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(128, 10)      # 10 classes\n",
        "        self.pool1 =nn.MaxPool2d(2, 2)   # kernel, stride\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.drop1 =nn.Dropout(0.20)\n",
        "        self.drop2 = nn.Dropout(0.20)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolution phase\n",
        "        z = self.relu(self.bn1(self.conv1(x)))  # Size([bs, 32, 24, 24])\n",
        "        z = self.pool1(z)                       # Size([bs, 32, 12, 12])\n",
        "        z = self.drop1(z)\n",
        "        z = self.relu(self.bn2(self.conv2(z)))  # Size([bs, 64, 8, 8])\n",
        "        z = self.pool2(z)                       # Size([bs, 64, 4, 4])\n",
        "\n",
        "        # neural network phase\n",
        "        z = z.reshape(-1, 1024)     # Size([bs, 1024])\n",
        "        z = self.relu(self.bn3(self.fc1(z)))     # Size([bs, 512])\n",
        "        z = self.drop2(z)\n",
        "        z = self.relu(self.bn4(self.fc2(z)))    # Size([bs, 256])\n",
        "        z = self.relu(self.bn5(self.fc3(z)))\n",
        "        #z = self.relu(self.fc4(z))\n",
        "        z = self.fc5(z)\n",
        "        return z\n",
        "\n",
        "# Create an instance of the nn.module class defined above:\n",
        "net = BaseNet()\n",
        "\n",
        "# Test your BaseNet with some random input\n",
        "dummy_input = torch.rand((1, 1, 28, 28))\n",
        "output = net(dummy_input)\n",
        "assert output.shape == torch.Size([1, 10])\n",
        "\n",
        "# For training on GPU, we need to transfer net and data onto the GPU\n",
        "# http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n",
        "if IS_GPU:\n",
        "    net = net.cuda()\n",
        "########################################################################\n",
        "# We provide a basic network that you should understand, run and\n",
        "# eventually improve\n",
        "# <<TODO>> Add more conv layers\n",
        "# <<TODO>> Add more fully connected (fc) layers\n",
        "# <<TODO>> Add regularization layers like Batchnorm.\n",
        "#          nn.BatchNorm2d after conv layers:\n",
        "#          http://pytorch.org/docs/master/nn.html#batchnorm2d\n",
        "#          nn.BatchNorm1d after fc layers:\n",
        "#          http://pytorch.org/docs/master/nn.html#batchnorm1d\n",
        "# This is a good resource for developing a CNN for classification:\n",
        "# http://cs231n.github.io/convolutional-networks/#layers\n",
        "\n",
        "# TODO: paste output in your report\n",
        "print(net)\n",
        "\n",
        "\"\"\"4.**Define a loss function and optimizer**\"\"\"\n",
        "\n",
        "########################################################################\n",
        "# Here we use Cross-Entropy loss and SGD with momentum.\n",
        "# The CrossEntropyLoss criterion already includes softmax within its\n",
        "# implementation. That's why we don't use a softmax in our model\n",
        "# definition.\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Tune the learning rate.\n",
        "# See whether the momentum and weight decay is useful or not\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0)\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "\"\"\"5.**Train the model**\"\"\"\n",
        "\n",
        "########################################################################\n",
        "# We simply have to loop over our data iterator, and feed the inputs to the\n",
        "# network and optimize. We evaluate the validation accuracy at each\n",
        "# epoch and plot these values over the number of epochs\n",
        "# Nothing to change here\n",
        "# -----------------------------\n",
        "plt.ioff()\n",
        "fig = plt.figure()\n",
        "train_loss_over_epochs = []\n",
        "val_accuracy_over_epochs = []\n",
        "import pdb\n",
        "for epoch in tqdm(range(EPOCHS), total=EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        if IS_GPU:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            #pdb.set_trace()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        net = net.cuda()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    # Normalizing the loss by the total number of train batches\n",
        "    running_loss/=len(trainloader)\n",
        "    print('[%d] loss: %.3f' %\n",
        "          (epoch + 1, running_loss))\n",
        "\n",
        "    # Scale of 0.0 to 100.0\n",
        "    # Calculate validation set accuracy of the existing model\n",
        "    val_accuracy, val_classwise_accuracy = \\\n",
        "        calculate_accuracy(valloader, net, IS_GPU)\n",
        "    print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
        "\n",
        "    # # Optionally print classwise accuracies\n",
        "    # for c_i in range(TOTAL_CLASSES):\n",
        "    #     print('Accuracy of %5s : %2d %%' % (\n",
        "    #         classes[c_i], 100 * val_classwise_accuracy[c_i]))\n",
        "\n",
        "    train_loss_over_epochs.append(running_loss)\n",
        "    val_accuracy_over_epochs.append(val_accuracy)\n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "# Plot train loss over epochs and val set accuracy over epochs\n",
        "# Nothing to change here\n",
        "# -------------\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.ylabel('Train loss')\n",
        "plt.plot(np.arange(EPOCHS), train_loss_over_epochs, 'k-')\n",
        "plt.title('train loss and val accuracy')\n",
        "plt.xticks(np.arange(EPOCHS, dtype=int))\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(np.arange(EPOCHS), val_accuracy_over_epochs, 'b-')\n",
        "plt.ylabel('Val accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.xticks(np.arange(EPOCHS, dtype=int))\n",
        "plt.grid(True)\n",
        "plt.savefig(\"mp4_q1_plot.png\")\n",
        "plt.close(fig)\n",
        "print('Finished Training')\n",
        "# -------------\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(net, input_size=(1, 28, 28))\n",
        "\n",
        "\"\"\"6.**Evaluate the validation accuracy of your final model**\"\"\"\n",
        "\n",
        "val_accuracy, val_classwise_accuracy = \\\n",
        "        calculate_accuracy(valloader, net, IS_GPU)\n",
        "print('Accuracy of the final network on the val images: %.1f %%' % (val_accuracy))\n",
        "\n",
        "# Optionally print classwise accuracies\n",
        "for c_i in range(TOTAL_CLASSES):\n",
        "    print('Accuracy of %5s : %.1f %%' % (\n",
        "        classes[c_i], val_classwise_accuracy[c_i]))\n",
        "\n",
        "\"\"\"7.**Visualize test set images**\"\"\"\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(testloader)\n",
        "images = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:16]))\n",
        "\n",
        "\"\"\"8.**Evaluate your final model on the test set**\n",
        "\n",
        "Submit `predictions.npy` to Gradescope to see your model's performance on the test set.\n",
        "\"\"\"\n",
        "\n",
        "# run inference on the test set\n",
        "predictions = run_secret_test(testloader, net, IS_GPU)\n",
        "# save predictions\n",
        "predictions = np.asarray(predictions)\n",
        "np.save(\"Q1_label_predictions.npy\", predictions)\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class BaseNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(BaseNet, self).__init__()\n",
        "\n",
        "#         # First convolutional layer\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         # Second convolutional layer\n",
        "#         self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=12)\n",
        "#         self.relu3 = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(in_features=12, out_features=10)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # TODO: define your model here\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.relu1(x)\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         # Second convolutional layer\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.relu2(x)\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         # Flatten for fully connected layers\n",
        "#         x = x.view(-1, 12 * 4 * 4)\n",
        "\n",
        "#         # Fully connected layers\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu3(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"CS543_MP4_part2_starter_code.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1cQWXsIuskrEvRPklplnXBtA5S7PMLh5H\n",
        "\"\"\"\n",
        "\n",
        "# Mounting your Google Drive is optional, and you could also simply copy and\n",
        "# upload the data to your colab instance. This manula upload is also easy to do,\n",
        "# but you will have to figure out how to do it.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"//content/gdrive/MyDrive/Datasets\"):\n",
        "    os.makedirs(\"/content/gdrive/My Drive/Colab Notebooks/CS_543_MP4\")\n",
        "os.chdir(\"//content/gdrive/MyDrive/Datasets\")\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# download dataset\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/Datasets/data\"):\n",
        "#   %pip install -U gdown\n",
        "  import gdown\n",
        "  url = \"https://drive.google.com/uc?id=1sdmNN6b3stiDCwyZbVsl5vS5VbPnKixa\"\n",
        "  gdown.download(url, quiet=False)\n",
        "  !unzip -qqo data.zip\n",
        "  !rm data.zip\n",
        "\n",
        "!pwd\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import models\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "\n",
        "# global variable\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# class SegmentationDataset(data.Dataset):\n",
        "#     \"\"\"\n",
        "#     Data loader for the Segmentation Dataset. If data loading is a bottleneck,\n",
        "#     you may want to optimize this in for faster training. Possibilities include\n",
        "#     pre-loading all images and annotations into memory before training, so as\n",
        "#     to limit delays due to disk reads.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, split=\"train\", data_dir=\"data\"):\n",
        "#         assert (split in [\"train\", \"val\", \"test\"])\n",
        "#         self.img_dir = os.path.join(data_dir, split)\n",
        "#         self.classes = []\n",
        "#         with open(os.path.join(data_dir, 'classes.txt'), 'r') as f:\n",
        "#             for l in f:\n",
        "#                 self.classes.append(l.rstrip())\n",
        "#         self.n_classes = len(self.classes)\n",
        "#         self.split = split\n",
        "#         self.data = glob.glob(self.img_dir + '/*.jpg')\n",
        "#         self.data = sorted([os.path.splitext(l)[0] for l in self.data])\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         img = Image.open(self.data[index] + '.jpg')\n",
        "#         if self.split == 'test':\n",
        "#             gt = Image.new('RGB', img.size)\n",
        "#         else:\n",
        "#             gt = Image.open(self.data[index] + '.png')\n",
        "#         img = ToTensor()(img)\n",
        "#         img = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
        "#         gt = np.asarray(gt)\n",
        "#         gt = torch.from_numpy(np.array(gt)).long().unsqueeze(0)\n",
        "#         return img, gt\n",
        "\n",
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision import transforms\n",
        "class SegmentationDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Data loader for the Segmentation Dataset. If data loading is a bottleneck,\n",
        "    you may want to optimize this in for faster training. Possibilities include\n",
        "    pre-loading all images and annotations into memory before training, so as\n",
        "    to limit delays due to disk reads.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, split=\"train\", data_dir=\"data\", input_size=(512, 512)):\n",
        "        assert (split in [\"train\", \"val\", \"test\"])\n",
        "        self.img_dir = os.path.join(data_dir, split)\n",
        "        self.classes = []\n",
        "        with open(os.path.join(data_dir, 'classes.txt'), 'r') as f:\n",
        "            for l in f:\n",
        "                self.classes.append(l.rstrip())\n",
        "        self.n_classes = len(self.classes)\n",
        "        self.split = split\n",
        "        self.data = glob.glob(self.img_dir + '/*.jpg')\n",
        "        self.data = sorted([os.path.splitext(l)[0] for l in self.data])\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.data[index] + '.jpg')\n",
        "        if self.split == 'test':\n",
        "            gt = Image.new('RGB', img.size)\n",
        "        else:\n",
        "            gt = Image.open(self.data[index] + '.png')\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.5:\n",
        "            img = F.hflip(img)\n",
        "            gt = F.hflip(gt)\n",
        "\n",
        "        # # Random cropping\n",
        "        # i, j, h, w = transforms.RandomCrop.get_params(\n",
        "        #     img, output_size=self.input_size)\n",
        "        # img = F.crop(img, i, j, h, w)\n",
        "        # gt = F.crop(gt, i, j, h, w)\n",
        "\n",
        "        img = ToTensor()(img)\n",
        "        img = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
        "        gt = np.asarray(gt)\n",
        "        gt = torch.from_numpy(np.array(gt)).long().unsqueeze(0)\n",
        "        return img, gt\n",
        "\n",
        "train_dataset = SegmentationDataset(split='train')\n",
        "\n",
        "# vis the training set\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(22, 7))\n",
        "for idx, ax_i in enumerate(axs.T):\n",
        "    ax = ax_i[0]\n",
        "    img, gt = train_dataset[idx]\n",
        "    img = std * img.permute((1, 2, 0)).cpu().numpy() + mean\n",
        "    gt = gt.squeeze().numpy()\n",
        "    ax.imshow((img * 255).astype(np.uint8))\n",
        "    ax.axis('off')\n",
        "    ax = ax_i[1]\n",
        "    ax.imshow(gt)\n",
        "    ax.axis('off')\n",
        "fig.tight_layout()\n",
        "plt.savefig('vis_trainset.pdf', format='pdf', bbox_inches='tight')\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "##########\n",
        "#TODO: design your own network here. The expectation is to write from scratch. But it's okay to get some inspiration\n",
        "#from conference paper. We are providing a very simple network that does a single 1x1 convolution to prdict the class label.\n",
        "##########\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes): # modify model\n",
        "        super(MyModel, self).__init__()\n",
        "        self.res18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        self.encoder = nn.Sequential(*list(self.res18.children())[:-2])\n",
        "        self.layers = nn.Sequential(nn.Conv2d(512, n_classes, 1, padding=0))\n",
        "        # self.up=nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, img): # modify forward path\n",
        "        #x = self.res18(img)\n",
        "        x=self.encoder(img)\n",
        "        #print(x.shape)\n",
        "        x=self.layers(x)\n",
        "        # x=self.up(x)\n",
        "        x = torch.nn.functional.interpolate(x, (224, 224), mode='bilinear')\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet18 model\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the fully connected and average pooling layers\n",
        "        modules = list(resnet18.children())[:-2]\n",
        "\n",
        "        # Add atrous convolutional layers for semantic segmentation\n",
        "        modules += [\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, n_classes, kernel_size=1, stride=1),\n",
        "            nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "        ]\n",
        "\n",
        "        # Combine the encoder and decoder into a single model\n",
        "        self.resnet_segmentation = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.resnet_segmentation(img)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet18 model\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the fully connected and average pooling layers\n",
        "        modules = list(resnet18.children())[:-2]\n",
        "\n",
        "        # Add atrous convolutional layers for semantic segmentation\n",
        "        modules += [\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, n_classes, kernel_size=1, stride=1),\n",
        "            nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)\n",
        "        ]\n",
        "\n",
        "        # Combine the encoder and decoder into a single model\n",
        "        self.resnet_segmentation = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.resnet_segmentation(img)\n",
        "        return x\n",
        "\n",
        "#[32, 3, 224, 224] image shape\n",
        "#[32, 1, 224, 224] target shape\n",
        "# [32, 6, 32, 32]  my output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet18 model\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the fully connected and average pooling layers\n",
        "        modules = list(resnet18.children())[:-2]\n",
        "\n",
        "        # Add atrous convolutional layers for semantic segmentation\n",
        "        modules += [\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, n_classes, kernel_size=1, stride=1),\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
        "        ]\n",
        "\n",
        "        # Combine the encoder and decoder into a single model\n",
        "        self.resnet_segmentation = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.resnet_segmentation(img)\n",
        "        return x\n",
        "\n",
        "#[32, 3, 224, 224] image shape\n",
        "#[32, 1, 224, 224] target shape\n",
        "# [32, 6, 32, 32]  my output\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet18 model\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the fully connected and average pooling layers\n",
        "        modules = list(resnet18.children())[:-2]\n",
        "\n",
        "        # Add atrous convolutional layers for semantic segmentation\n",
        "        modules += [\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, n_classes, kernel_size=1, stride=1),\n",
        "            nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)\n",
        "        ]\n",
        "\n",
        "        # Combine the encoder and decoder into a single model\n",
        "        self.resnet_segmentation = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.resnet_segmentation(img)\n",
        "        return x\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet18 model\n",
        "        resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove the fully connected and average pooling layers\n",
        "        modules = list(resnet18.children())[:-2]\n",
        "\n",
        "        # Add atrous convolutional layers for semantic segmentation\n",
        "        modules += [\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, n_classes, kernel_size=1, stride=1),\n",
        "            nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)\n",
        "        ]\n",
        "\n",
        "        # Combine the encoder and decoder into a single model\n",
        "        self.resnet_segmentation = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.resnet_segmentation(img)\n",
        "        return x\n",
        "\n",
        "# ##########\n",
        "# #TODO: define your loss function here, we provide the basic Cross Entropy Loss\n",
        "# ##########\n",
        "import torch.nn as nn\n",
        "\n",
        "# class MyCriterion(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(MyCriterion, self).__init__()\n",
        "#         #self.n_classes = n_classes\n",
        "#         self.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "#     def forward(self, prediction, target):\n",
        "#         #print(\"target\",target.shape)\n",
        "#         loss = self.criterion(prediction, target.squeeze())\n",
        "\n",
        "#         return loss\n",
        "\n",
        "class MyCriterion(nn.Module):\n",
        "    def __init__(self, n_classes, class_weights=None):\n",
        "        super(MyCriterion, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.class_weights = class_weights\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=255, weight=self.class_weights)\n",
        "\n",
        "    def forward(self, prediction, target):\n",
        "        # Calculate the class weights if not provided\n",
        "        if self.class_weights is None:\n",
        "            target_flat = target.view(-1)\n",
        "            class_counts = torch.bincount(target_flat, minlength=self.n_classes)\n",
        "            class_weights = 1.0 / class_counts.float()\n",
        "            self.criterion.weight = class_weights.to(prediction.device)\n",
        "\n",
        "        loss = self.criterion(prediction, target.squeeze())\n",
        "\n",
        "        return loss\n",
        "\n",
        "########################################################################\n",
        "# No need to modify below\n",
        "# Evaluate sementic segmentation\n",
        "# 1. Average precision of all classes and the average\n",
        "# 2. Mean IOU of all classes and the average\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def segmentation_eval(gts, preds, classes):\n",
        "    \"\"\"\n",
        "    @param    gts               numpy.ndarray   ground truth labels\n",
        "    @param    preds             numpy.ndarray   predicted labels\n",
        "    @param    classes           string          class names\n",
        "    \"\"\"\n",
        "    ious, counts = compute_confusion_matrix(gts, preds)\n",
        "    aps = compute_ap(gts, preds)\n",
        "    for i in range(len(classes)):\n",
        "        print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format(classes[i], aps[i], ious[i]))\n",
        "    print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format('mean', np.mean(aps), np.mean(ious)))\n",
        "    return aps, ious\n",
        "\n",
        "\n",
        "def compute_ap(gts, preds):\n",
        "    aps = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        ap, prec, rec = calc_pr(gts == i, preds[:, i:i + 1, :, :])\n",
        "        aps.append(ap)\n",
        "    return aps\n",
        "\n",
        "\n",
        "def calc_pr(gt, out, wt=None):\n",
        "    gt = gt.astype(np.float64).reshape((-1, 1))\n",
        "    out = out.astype(np.float64).reshape((-1, 1))\n",
        "\n",
        "    tog = np.concatenate([gt, out], axis=1) * 1.\n",
        "    ind = np.argsort(tog[:, 1], axis=0)[::-1]\n",
        "    tog = tog[ind, :]\n",
        "    cumsumsortgt = np.cumsum(tog[:, 0])\n",
        "    cumsumsortwt = np.cumsum(tog[:, 0] - tog[:, 0] + 1)\n",
        "    prec = cumsumsortgt / cumsumsortwt\n",
        "    rec = cumsumsortgt / np.sum(tog[:, 0])\n",
        "    ap = voc_ap(rec, prec)\n",
        "    return ap, rec, prec\n",
        "\n",
        "\n",
        "def voc_ap(rec, prec):\n",
        "    rec = rec.reshape((-1, 1))\n",
        "    prec = prec.reshape((-1, 1))\n",
        "    z = np.zeros((1, 1))\n",
        "    o = np.ones((1, 1))\n",
        "    mrec = np.vstack((z, rec, o))\n",
        "    mpre = np.vstack((z, prec, z))\n",
        "\n",
        "    mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n",
        "    I = np.where(mrec[1:] != mrec[0:-1])[0] + 1\n",
        "    ap = np.sum((mrec[I] - mrec[I - 1]) * mpre[I])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def compute_confusion_matrix(gts, preds):\n",
        "    preds_cls = np.argmax(preds, 1)\n",
        "    gts = gts[:, 0, :, :]\n",
        "    conf = confusion_matrix(gts.ravel(), preds_cls.ravel())\n",
        "    inter = np.diag(conf)\n",
        "    union = np.sum(conf, 0) + np.sum(conf, 1) - np.diag(conf)\n",
        "    union = np.maximum(union, 1)\n",
        "    return inter / union, conf\n",
        "\n",
        "\n",
        "def val(model, val_dataloader, device):\n",
        "    preds, gts = [], []\n",
        "\n",
        "    # Put model in evaluation mode.\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        img, gt = batch\n",
        "        img = img.to(device)\n",
        "        gt = gt.to(device).long()\n",
        "        pred = model(img)\n",
        "        pred = torch.softmax(pred, 1)\n",
        "        preds.append(pred.detach().cpu().numpy())\n",
        "        gts.append(gt.detach().cpu().numpy())\n",
        "    gts = np.concatenate(gts, 0)\n",
        "    preds = np.concatenate(preds, 0)\n",
        "    aps, ious = segmentation_eval(gts, preds, val_dataset.classes)\n",
        "\n",
        "    # Put model back in training mode\n",
        "    model.train()\n",
        "    return preds\n",
        "\n",
        "# TODO: implement your train loop here, we provide a very basic training loop\n",
        "def train(model, criterion, optimizer, train_dataloader, epoch, **kwargs):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        # Zero out gradient blobs in the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        img, gt = batch\n",
        "\n",
        "        # Move data to device for training\n",
        "        img = img.to(device)\n",
        "        gt = gt.to(device).long()\n",
        "\n",
        "        # Get model predictions\n",
        "        pred = model(img)\n",
        "        loss = criterion(pred, gt)\n",
        "        loss.backward()\n",
        "\n",
        "        # Take a step to update network parameters.\n",
        "        optimizer.step()\n",
        "\n",
        "# TODO: Implement your training cycles, make sure you evaluate on validation\n",
        "# dataset and compute evaluation metrics every so often.\n",
        "# You may also want to save models that perform well.\n",
        "# Tune your own optimizer and number of epochs, learning rate, etc\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "model = MyModel(n_classes=6).to(device)\n",
        "criterion = MyCriterion(n_classes=6).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=32,\n",
        "                                    shuffle=True, num_workers=2,\n",
        "                                    drop_last=True)\n",
        "\n",
        "val_dataset = SegmentationDataset(split=\"val\")\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=1,\n",
        "                                 shuffle=False, num_workers=0,\n",
        "                                 drop_last=False)\n",
        "num_epochs = 40\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train(model, criterion, optimizer, train_dataloader, epoch)\n",
        "    # consider reducing learning rate\n",
        "    #scheduler.step()\n",
        "    # test results on validation set\n",
        "    if epoch % 10 ==0 or epoch == num_epochs-1:\n",
        "        print(\"epoch: {}, performance on validation set\".format(epoch))\n",
        "        preds = val(model, val_dataloader, device)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 224, 224))\n",
        "\n",
        "# visualization pred on validation set against GT\n",
        "# Feel free to modify for your custom visualization\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "# vis input & pred on test set\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(22, 7))\n",
        "for idx, ax_i in enumerate(axs.T):\n",
        "    ax = ax_i[0]\n",
        "    img, gt = val_dataset[idx]\n",
        "    img = std * img.permute((1, 2, 0)).cpu().numpy() + mean\n",
        "    ax.imshow((img * 255).astype(np.uint8))\n",
        "    ax.axis('off')\n",
        "    ax = ax_i[1]\n",
        "    pred = np.argmax(preds[idx], 0)\n",
        "    ax.imshow(pred)\n",
        "    ax.axis('off')\n",
        "fig.tight_layout()\n",
        "plt.savefig('vis_valset.pdf', format='pdf', bbox_inches='tight')\n",
        "\n",
        "########################################################################\n",
        "# No need to modify below\n",
        "# Generate predictions on test split\n",
        "def predict(model, test_dataloader, device):\n",
        "    preds = []\n",
        "    # Put model in evaluation mode.\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(test_dataloader):\n",
        "        img, _ = batch\n",
        "        img = img.to(device)\n",
        "        pred = model(img)\n",
        "        pred = torch.softmax(pred, 1)\n",
        "        preds.append(pred.detach().cpu().numpy())\n",
        "    preds = np.concatenate(preds, 0)\n",
        "    # Put model back in training mode\n",
        "    model.train()\n",
        "    return preds\n",
        "\n",
        "test_dataset = SegmentationDataset(split=\"test\")\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=False)\n",
        "preds = predict(model, test_dataloader, device)\n",
        "\n",
        "# visualization pred on test set\n",
        "# Feel free to pick your favorite images for visualization\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "# vis input & pred on test set\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(22, 7))\n",
        "for idx, ax_i in enumerate(axs.T):\n",
        "    ax = ax_i[0]\n",
        "    img, _ = test_dataset[idx]\n",
        "    img = std * img.permute((1, 2, 0)).cpu().numpy() + mean\n",
        "    ax.imshow((img * 255).astype(np.uint8))\n",
        "    ax.axis('off')\n",
        "    ax = ax_i[1]\n",
        "    pred = np.argmax(preds[idx], 0)\n",
        "    ax.imshow(pred)\n",
        "    ax.axis('off')\n",
        "fig.tight_layout()\n",
        "plt.savefig('vis_testset.pdf', format='pdf', bbox_inches='tight')\n",
        "\n",
        "# save prediction, please upload to Gradescope\n",
        "np.save('Q2_sseg_predictions', (preds*255).astype(np.uint8))\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "\n",
        "# # Load pre-trained ResNet-18 model\n",
        "# resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "# # Remove the classifier and global average pooling layers\n",
        "# modules = list(resnet.children())[:-2]\n",
        "# resnet = nn.Sequential(*modules)\n",
        "\n",
        "# # Add upsampling layers for semantic segmentation\n",
        "# num_classes = 2 # Example: Binary classification task\n",
        "# resnet_decoder = nn.Sequential(\n",
        "#     nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.ConvTranspose2d(64, num_classes, kernel_size=4, stride=2, padding=1),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "# )\n",
        "\n",
        "# # Combine the encoder and decoder into a single model\n",
        "# resnet_segmentation = nn.Sequential(resnet, resnet_decoder)\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "\n",
        "# # Load pre-trained ResNet-18 model\n",
        "# resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "# # Remove the classifier and global average pooling layers\n",
        "# modules = list(resnet.children())[:-2]\n",
        "# resnet = nn.Sequential(*modules)\n",
        "\n",
        "# # Add atrous convolutional layers for semantic segmentation\n",
        "# num_classes = 2 # Example: Binary classification task\n",
        "# resnet_decoder = nn.Sequential(\n",
        "#     nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(64, num_classes, kernel_size=1, stride=1),\n",
        "#     nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "# )\n",
        "\n",
        "# # Combine the encoder and decoder into a single model\n",
        "# resnet_segmentation = nn.Sequential(resnet, resnet_decoder)\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "\n",
        "# class ResNet18Encoder(nn.Module):\n",
        "#     def __init__(self, pretrained=True):\n",
        "#         super(ResNet18Encoder, self).__init__()\n",
        "#         self.encoder = models.resnet18(pretrained=pretrained)\n",
        "#         # remove the last two layers: AvgPool and FC\n",
        "#         self.encoder = nn.Sequential(*list(self.encoder.children())[:-2])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.encoder(x)\n",
        "\n",
        "# class AtrousConvolutionDecoder(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(AtrousConvolutionDecoder, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1)\n",
        "#         self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2, dilation=2)\n",
        "#         self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=4, dilation=4)\n",
        "#         self.conv4 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=8, dilation=8)\n",
        "#         self.conv5 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=16, dilation=16)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.conv1(x)\n",
        "#         x2 = self.conv2(x)\n",
        "#         x3 = self.conv3(x)\n",
        "#         x4 = self.conv4(x)\n",
        "#         x5 = self.conv5(x)\n",
        "#         return x1 + x2 + x3 + x4 + x5\n",
        "\n",
        "# class FCNDecoder(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_classes):\n",
        "#         super(FCNDecoder, self).__init__()\n",
        "#         self.deconv1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "#         self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "#         self.deconv2 = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "#         self.deconv3 = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "#         self.classifier = nn.Conv2d(out_channels, num_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.deconv1(x)\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.deconv2(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.deconv3(x)\n",
        "#         x = self.conv3(x)\n",
        "#         return self.classifier(x)\n",
        "\n",
        "# class ResNet18Segmentation(nn.Module):\n",
        "#     def __init__(self, num_classes, use_atrous=False, use_fcn=False):\n",
        "#         super(ResNet18Segmentation, self).__init__()\n",
        "#         self.encoder = ResNet18Encoder()\n",
        "#         if use_atrous:\n",
        "#             self.decoder = AtrousConvolutionDecoder(512, 256)\n",
        "#         elif use_fcn:\n",
        "#             self.decoder = FCNDecoder(512, 256, num_classes)\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# class MyModel(nn.Module):\n",
        "#   def __init__(self, n_classes): # modify model\n",
        "#     super(MyModel, self).__init__()\n",
        "# # Load pre-trained ResNet-18 model\n",
        "#     resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "#     print(resnet)\n",
        "#     # Remove the classifier and global average pooling layers\n",
        "#     modules = list(resnet.children())[:-2]\n",
        "#     #resnet = nn.Sequential(*modules)\n",
        "\n",
        "#     # Add atrous convolutional layers for semantic segmentation\n",
        "#     num_classes = 6 # Example: Binary classification task\n",
        "#     resnet_decoder = nn.Sequential(\n",
        "#         nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1, dilation=2),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv2d(64, num_classes, kernel_size=1, stride=1),\n",
        "#         nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
        "#     )\n",
        "\n",
        "#     # Combine the encoder and decoder into a single model\n",
        "#     self.resnet_segmentation = nn.Sequential(modules, resnet_decoder)\n",
        "#   def forward(self, img):\n",
        "#     x=self.resnet_segmentation(img)\n",
        "#     return x"
      ],
      "metadata": {
        "id": "-RJwM0-NpUmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}